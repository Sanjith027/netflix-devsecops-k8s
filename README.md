# Netflix Clone | End-to-End DevSecOps CI/CD Pipeline with Security Scanning and Kubernetes on AWS

![Architecture Diagram](assets/arch-diag.gif)

A **complete end-to-end DevSecOps project** showcasing how to automate, secure, and monitor infrastructure and applications using modern tools â€” from **Terraform to Kubernetes**, **Jenkins to Trivy**, and everything in between.

Built to demonstrate **real-world DevSecOps workflows** for CI/CD, cloud automation, security integration, and observability â€” all in one Netflix-themed application. ğŸ¿

---

## ğŸ“‹ Table of Contents

- [ğŸš€ Project Overview](#-project-overview)
- [ğŸŒ Key Features](#-key-features)
- [ğŸ§© Directory Structure](#-directory-structure)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Infrastructure Automation with Terraform Cloud & GitHub Actions](#-infrastructure-automation-with-terraform-cloud--github-actions)
  - [ğŸ” Terraform Cloud Authentication](#-terraform-cloud-authentication)
  - [âš™ï¸ Automating Infrastructure with GitHub Actions](#ï¸-automating-infrastructure-with-github-actions)
  - [ğŸš€ Infrastructure Deployment Flow](#-infrastructure-deployment-flow)
- [ğŸ› ï¸ Toolchain Overview](#ï¸-toolchain-overview)
- [âš™ï¸ Jenkins Server Setup](#ï¸-jenkins-server-setup)
- [ğŸ³ Docker Installation on Jenkins](#-docker-installation-on-jenkins)
- [ğŸ§  SonarQube Setup (Code Quality)](#-sonarqube-setup-code-quality)
- [ğŸ›¡ï¸ Trivy Installation (Security Scanning)](#ï¸-trivy-installation-security-scanning)
- [âš™ï¸ kubectl Installation](#ï¸-kubectl-installation)
- [ğŸ”” Slack Notifications Setup](#-slack-notifications-setup)
- [ğŸ› ï¸ Jenkins Plugin Installation](#ï¸-jenkins-plugin-installation)
- [âš™ï¸ Plugin Configuration](#ï¸-plugin-configuration)
  - [ğŸŸ¢ NodeJS Configuration](#-nodejs-configuration)
  - [ğŸ§  SonarQube Integration](#-sonarqube-integration)
  - [ğŸš¦ Quality Gate Webhook Setup](#-quality-gate-webhook-setup)
  - [ğŸ“Š SonarQube Project Setup](#-sonarqube-project-setup)
- [ğŸ§© Jenkins Pipeline Creation](#-jenkins-pipeline-creation)
- [ğŸ›¡ï¸ OWASP Dependency Check Integration](#ï¸-owasp-dependency-check-integration)
- [ğŸ³ Docker Integration for Image Build & Push](#-docker-integration-for-image-build--push)
- [ğŸ”” Pipeline Visibility](#-pipeline-visibility)
- [ğŸ¬ TMDB API Key Setup](#-tmdb-api-key-setup)
- [ğŸ³ Docker Configuration & Pipeline Execution](#-docker-configuration--pipeline-execution)
- [ğŸ—ï¸ Kubernetes Cluster Setup Using kubeadm](#ï¸-kubernetes-cluster-setup-using-kubeadm)
- [ğŸ”— Integrating Kubernetes with Jenkins](#-integrating-kubernetes-with-jenkins)
- [ğŸš€ Re-running the Jenkins Pipeline](#-re-running-the-jenkins-pipeline)
- [ğŸŒ Application Access](#-application-access)
- [ğŸ—ï¸ Monitoring Architecture](#ï¸-monitoring-architecture)
  - [âš™ï¸ Prometheus Setup](#ï¸-prometheus-setup-monitoring-server)
  - [ğŸ§© Node Exporter Setup](#-node-exporter-setup-all-servers)
  - [ğŸ”„ Prometheus Scrape Configuration](#-prometheus-scrape-configuration)
  - [ğŸ“ˆ Grafana Setup](#-grafana-setup-visualization-layer)
  - [ğŸ”” Jenkins Monitoring](#-jenkins-monitoring)
  - [â˜¸ï¸ Kubernetes Monitoring](#ï¸-kubernetes-monitoring)
- [âœ… Final Result](#-final-result)

---

## ğŸš€ Project Overview

This project simulates a real enterprise-grade setup where a **React-based Netflix Clone** is deployed and managed through a **secure, automated DevOps pipeline**.

---

## ğŸŒ Key Features

- **Infrastructure as Code** with Terraform (AWS provisioning)
- **State management** using Terraform Cloud
- **CI/CD automation** with GitHub Actions and Jenkins
- **Security Scanning** with Trivy & OWASP Dependency Check
- **Containerization** with Docker
- **Kubernetes Deployment** (unmanaged cluster setup)
- **Monitoring Stack** for Jenkins, Kubernetes, and the app itself

---

## ğŸ§© Directory Structure

```bash
.
â”œâ”€â”€ Application-Code        # Frontend Netflix Clone app built with React + Vite
â”‚   â”œâ”€â”€ Dockerfile           # Docker image build instructions
â”‚   â”œâ”€â”€ package.json         # Dependencies and scripts
â”‚   â”œâ”€â”€ src/                 # Main source code
â”‚   â””â”€â”€ public/              # Static assets
â”‚
â”œâ”€â”€ Jenkins
â”‚   â””â”€â”€ Jenkinsfile          # CI/CD pipeline configuration (build â†’ test â†’ deploy)
â”‚
â”œâ”€â”€ Kubernetes
â”‚   â”œâ”€â”€ deployment.yml       # App deployment manifest
â”‚   â””â”€â”€ service.yml          # K8s service exposure
â”‚
â””â”€â”€ Terraform
    â”œâ”€â”€ main.tf              # AWS resource definitions
    â”œâ”€â”€ backend.tf           # Terraform Cloud backend configuration
    â”œâ”€â”€ iam.tf               # IAM roles and policies
    â”œâ”€â”€ vpc.tf               # Network setup
    â”œâ”€â”€ variables.tf         # Input variables
    â”œâ”€â”€ dev.auto.tfvars      # Environment variables
    â””â”€â”€ gather.tf            # Data sources and dependencies
```

---

## ğŸ› ï¸ Tech Stack

| Category | Tools / Technologies |
|-----------|----------------------|
| **Infrastructure** | Terraform, AWS EC2, Terraform Cloud |
| **CI/CD** | Jenkins, GitHub Actions |
| **Security** | Trivy, SonarQube, OWASP Dependency Check |
| **Containerization** | Docker |
| **Orchestration** | Kubernetes (Unmanaged Cluster) |
| **Monitoring** | Node Exporter, Prometheus, Kube State Metrics |
| **Frontend** | React, Vite, TMDB API |

---

## ğŸš€ Infrastructure Automation with Terraform Cloud & GitHub Actions

To automate AWS infrastructure provisioning, I integrated Terraform Cloud with GitHub Actions, enabling secure, remote, and fully automated Terraform operations as part of the CI/CD workflow.

### ğŸ” Terraform Cloud Authentication

I created an API token in Terraform Cloud and securely stored it as a GitHub Actions secret. This allows GitHub Actions to authenticate with Terraform Cloud while running Terraform commands such as init, plan, and apply, without exposing sensitive credentials in the repository.

### âš™ï¸ Automating Infrastructure with GitHub Actions

I configured a GitHub Actions workflow to automatically manage the AWS infrastructure using Terraform. The pipeline performs the following actions:

- Initializes Terraform using Terraform Cloud as the remote backend
- Executes terraform plan to validate infrastructure changes
- Applies the configuration using terraform apply after approval
- Provisions AWS resources in a consistent and repeatable manner

This setup ensures that infrastructure changes are version-controlled, auditable, and automated.

### ğŸš€ Infrastructure Deployment Flow

With Terraform, Terraform Cloud, and GitHub Actions integrated, the infrastructure deployment process is fully automated on AWS.

To deploy the infrastructure:

1. Navigate to the Actions tab in the GitHub repository
2. Select the Terraform workflow
3. Choose Apply from the dropdown
4. Trigger the workflow to provision or update AWS resources

Once triggered, GitHub Actions securely connects to Terraform Cloud and deploys the infrastructure on AWS.

![Terraform Cloud](assets/terraform-cloud.png)

![EC2 Instances](assets/ec2.png)

---

## ğŸ› ï¸ Toolchain Overview

- **Jenkins** â€” CI/CD orchestrator
- **Docker** â€” Container runtime for isolated and repeatable builds
- **SonarQube** â€” Static code analysis and code quality gates
- **Trivy** â€” Vulnerability scanning for images and dependencies
- **kubectl** â€” CLI utility for upcoming Kubernetes deployments
- **Slack Notifications** â€” Pipeline status alerts (success/failure)

---

## âš™ï¸ Jenkins Server Setup

I started by connecting to the Jenkins EC2 instance using AWS Session Manager and logged in as the ubuntu user.

Before installing Jenkins, I installed Java, which is a prerequisite.

Once Java was installed, I installed Jenkins, started the service, and verified the setup by accessing the Jenkins UI on port 8080 using the public IP of the server.

After retrieving the initial admin password, I:

- Installed the recommended Jenkins plugins
- Created a new Jenkins user instead of using the default admin account
- Completed the setup and validated that Jenkins was running successfully

At this point, the Jenkins server was fully operational.

---

## ğŸ³ Docker Installation on Jenkins

To enable Jenkins to build and run containers, I installed Docker on the Jenkins server.

I added both the jenkins and ubuntu users to the Docker group and restarted the Docker service. This allows Jenkins pipelines to execute Docker commands without permission issues.

Docker is used to:

- Build application images
- Run SonarQube as a container
- Scan images using Trivy

---

## ğŸ§  SonarQube Setup (Code Quality)

For static code analysis and quality checks, I deployed SonarQube using a Docker container instead of installing it directly on the EC2 instance.

Running SonarQube in Docker keeps the setup lightweight and easy to manage.

After starting the container, I accessed the SonarQube dashboard on port 9000, logged in using the default credentials, and updated the admin password.

SonarQube is later integrated into the Jenkins pipeline to:

- Analyze code quality
- Detect bugs and code smells
- Enforce quality gates before deployment

---

## ğŸ›¡ï¸ Trivy Installation (Security Scanning)

To introduce shift-left security, I installed Trivy on the Jenkins server.

Trivy is used in the pipeline to:

- Scan Docker images for vulnerabilities
- Detect vulnerable dependencies
- Fail builds if critical issues are found

This ensures security checks are part of the CI/CD workflow, not an afterthought.

---

## âš™ï¸ kubectl Installation

I installed kubectl on the Jenkins server to prepare for Kubernetes deployments.

This allows Jenkins pipelines to:

- Interact with Kubernetes clusters
- Deploy application manifests
- Manage resources programmatically

This step enables seamless CI/CD â†’ Kubernetes integration.

---

## ğŸ”” Slack Notifications Setup

To receive pipeline status updates, I integrated Jenkins with Slack.

Steps:

1. Install the Slack Notification plugin in Jenkins
2. Create a Slack App and generate a Bot User Token
3. Add the token as a Jenkins credential
4. Configure the workspace and channel in Jenkins
5. Invite the Jenkins bot to the Slack channel
6. Test and save the configuration

Jenkins now sends notifications for pipeline success, failure, and aborted runs.

---

## ğŸ› ï¸ Jenkins Plugin Installation

To support the pipeline stages, I installed the required Jenkins plugins:

- **SonarQube Scanner** â€” for integrating SonarQube scans
- **NodeJS** â€” to build and test the Node.js-based application

After installing the plugins, I restarted Jenkins to apply the changes.

---

## âš™ï¸ Plugin Configuration

### ğŸŸ¢ NodeJS Configuration

From Manage Jenkins â†’ Tools, I configured:

- A NodeJS installation with a suitable version for the application
- Enabled automatic installation
- Saved and applied the configuration

This allows Jenkins to build the React/Node-based Netflix Clone during pipeline execution.

### ğŸ§  SonarQube Integration

To enable static code analysis and quality gates, I integrated SonarQube with Jenkins.

**SonarQube Authentication:**
- Generated a SonarQube token from the SonarQube dashboard
- Stored the token securely as a Jenkins credential

**Jenkinsâ€“SonarQube Configuration:**
- Added SonarQube under Manage Jenkins â†’ System
- Configured the server name (sonar-server) and server URL
- Linked the previously added credentials
- Configured SonarQube Scanner under Manage Jenkins â†’ Tools

### ğŸš¦ Quality Gate Webhook Setup

To enable Jenkins to receive SonarQube Quality Gate results, I:

- Created a webhook in SonarQube
- Configured it to point to the Jenkins webhook endpoint

This ensures the pipeline reacts automatically based on code quality results.

### ğŸ“Š SonarQube Project Setup

On the SonarQube dashboard, I:

- Created a local project
- Selected JS/TS & Web (Node.js-based application)
- Reused the existing authentication token
- Copied the generated scan commands for Jenkins pipeline usage

---

## ğŸ§© Jenkins Pipeline Creation

I created a new Jenkins pipeline job and configured it to:

- Pull the pipeline script directly from GitHub
- Execute the pipeline stages defined in the repository

During the initial run, the pipeline failed at the OWASP Dependency Check stage, which was expected because the tool was not yet installed.

This mirrors a real-world DevSecOps scenario where missing security tooling causes pipeline failures.

---

## ğŸ›¡ï¸ OWASP Dependency Check Integration

To resolve the failure:

- Installed the OWASP Dependency Check Jenkins plugin
- Configured the tool under Manage Jenkins â†’ Tools
- Enabled automatic installation

After re-running the pipeline:

- Dependency scanning completed successfully
- Vulnerability reports were generated
- Slack notifications confirmed the pipeline status

---

## ğŸ³ Docker Integration for Image Build & Push

Since the pipeline includes Docker image creation:

- Ensured Docker was properly configured on Jenkins
- Generated a Docker Hub Personal Access Token
- Stored the credentials securely in Jenkins
- Installed the Docker plugin to support image build and push operations

This enables Jenkins to:

- Build Docker images
- Scan them using Trivy
- Push them to Docker Hub securely

---

## ğŸ”” Pipeline Visibility

Throughout the process:

- Pipeline failures and successes triggered Slack notifications
- SonarQube reports validated code quality
- OWASP reports confirmed dependency health

![SonarQube](assets/sonarqube.png)

![Jenkins Credentials](assets/jenkins-cred.png)

![Jenkins Dashboard](assets/jenkins.png)

![Slack Notifications](assets/slack.png)

![Dependency Check](assets/dependency-check.png)

---

## ğŸ¬ TMDB API Key Setup

To enable movie data in the Netflix Clone, I generated an API key from TMDB.

Steps:

1. Create an account at themoviedb.org
2. Go to Profile â†’ Settings â†’ API
3. Generate a new API key
4. Add the API key as a credential in Jenkins

---

## ğŸ³ Docker Configuration & Pipeline Execution

After adding the TMDB API key:

- Configured the Docker tool in Jenkins
- Ran the Jenkins pipeline again

The pipeline successfully:

- Built the Docker image
- Pushed the image to Docker Hub

---

## ğŸ—ï¸ Kubernetes Cluster Setup Using kubeadm

### 1ï¸âƒ£ Hostname Configuration

I first logged into both EC2 instances and set clear hostnames so Kubernetes could identify node roles.

**Master node:**
```bash
sudo hostnamectl set-hostname k8s-master
```

**Worker node:**
```bash
sudo hostnamectl set-hostname k8s-worker
```

### 2ï¸âƒ£ Kubernetes Version & System Preparation (Both Nodes)

I defined the Kubernetes version and prepared the OS to meet Kubernetes requirements.

Key things I did:

- Disabled swap (mandatory for Kubernetes)
- Loaded required kernel modules
- Enabled IP forwarding and bridge networking

This ensures pods can communicate across nodes.

### 3ï¸âƒ£ Container Runtime Setup (containerd)

Kubernetes needs a container runtime to run pods. I installed containerd on both nodes and configured it to use systemd cgroups, which Kubernetes expects.

After configuration:

- containerd was running
- enabled on boot
- ready to handle pod workloads

### 4ï¸âƒ£ Installing Kubernetes Components (Both Nodes)

On both Master and Worker nodes, I installed:

- **kubeadm** â†’ cluster bootstrap tool
- **kubelet** â†’ node agent
- **kubectl** â†’ cluster management CLI

I also locked the versions to avoid accidental upgrades.

### 5ï¸âƒ£ Initializing the Cluster (Master Node Only)

On the Master node, I initialized the cluster:

```bash
kubeadm init --pod-network-cidr=10.244.0.0/16
```

Why this matters:

- Initializes the control plane
- Generates cluster certificates
- Creates the join command for worker nodes
- Defines the pod network CIDR (required for Flannel)

### 6ï¸âƒ£ kubectl Configuration (Master Node)

After initialization, I configured kubectl so I could manage the cluster as a normal user:

- Copied admin.conf
- Set correct permissions

This allowed me to run commands like:
```bash
kubectl get nodes
kubectl get pods -A
```

### 7ï¸âƒ£ Installing Pod Network (Flannel)

Without a network plugin, pods cannot talk to each other.

I installed Flannel:

```bash
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

After this:

- Core system pods moved to Running
- Cluster networking became functional

### 8ï¸âƒ£ Joining the Worker Node

Using the kubeadm join command generated earlier, I joined the worker node to the cluster.

Once completed:

```bash
kubectl get nodes
```

Both master and worker nodes appeared as Ready.

At this point, the Kubernetes cluster was fully operational.

---

## ğŸ”— Integrating Kubernetes with Jenkins

**Why This Was Needed:**

Jenkins must authenticate with Kubernetes to deploy workloads.

To achieve this, I:

1. Retrieved the kubeconfig file from the master node:
   ```bash
   sudo cat /etc/kubernetes/admin.conf
   ```

2. Added this kubeconfig securely to Jenkins credentials

3. Configured Jenkins pipelines to use kubectl with this config

This allowed Jenkins to:

- Apply Kubernetes manifests
- Create Deployments and Services
- Verify rollout status

---

## ğŸš€ Re-running the Jenkins Pipeline

Once Kubernetes integration was complete, I re-ran the Jenkins pipeline.

This time:

- Docker image was built and pushed
- Jenkins successfully connected to Kubernetes
- Deployment manifests were applied
- Pods were created and started successfully

I validated the deployment using:

```bash
kubectl get all -n default
```

---

## ğŸŒ Application Access

After confirming all pods and services were running:

- The Netflix Clone application became accessible
- The CI/CD pipeline finally completed end-to-end without failure

![Kubernetes Pods](assets/pods.png)

---

## ğŸ—ï¸ Monitoring Architecture (How I Designed It)

I followed a centralized pull-based monitoring model:

- Prometheus runs on one Monitoring EC2 instance
- Node Exporter runs on every server that needs monitoring
- Prometheus scrapes metrics from all exporters
- Grafana visualizes everything using dashboards

This is a standard production-grade monitoring design.

---

## âš™ï¸ Prometheus Setup (Monitoring Server)

### 1ï¸âƒ£ System Preparation

I updated the system and created a dedicated system user for Prometheus to follow security best practices.

Why:

- Prometheus should not run as root
- Limits blast radius if compromised

I also created required directories:

- `/etc/prometheus` â†’ configuration
- `/var/lib/prometheus` â†’ metrics storage

### 2ï¸âƒ£ Installing Prometheus

I downloaded the official Prometheus binary, extracted it, and:

- Moved the prometheus and promtool binaries to `/usr/local/bin`
- Placed prometheus.yml in `/etc/prometheus`
- Assigned ownership to the prometheus user

This gives full control without relying on package managers.

### 3ï¸âƒ£ Prometheus as a System Service

I created a systemd service for Prometheus so that:

- It starts automatically on reboot
- Can be managed via systemctl
- Runs reliably in the background

After enabling and starting the service, I validated:

- Prometheus service status
- Prometheus UI on port 9090

At this point, Prometheus was running but not scraping anything yet.

---

## ğŸ§© Node Exporter Setup (All Servers)

**Why Node Exporter?**

Node Exporter exposes OS-level metrics, such as:

- CPU usage
- Memory utilization
- Disk I/O
- Network traffic

This is essential for infrastructure monitoring.

**Node Exporter Installation Flow:**

I followed the same approach on:

- Monitoring Server
- Jenkins Server
- Kubernetes Master
- Kubernetes Worker

For each server:

1. Created a node_exporter system user
2. Downloaded and extracted Node Exporter
3. Moved the binary to `/usr/local/bin`
4. Created a systemd service
5. Enabled and started the service

Each node now exposed metrics on:

```
http://<node-ip>:9100/metrics
```

I verified metrics using curl before integrating with Prometheus.

---

## ğŸ”„ Prometheus Scrape Configuration

Once exporters were running, I configured Prometheus to scrape them.

In prometheus.yml, I added separate scrape jobs for:

- Prometheus itself
- Node Exporter on Monitoring Server
- Jenkins Server
- Kubernetes Master Node
- Kubernetes Worker Node

After updating the config:

1. Validated it using promtool
2. Restarted Prometheus
3. Confirmed all targets were UP in the Prometheus UI

This confirmed metrics were flowing correctly.

---

## ğŸ“ˆ Grafana Setup (Visualization Layer)

### 1ï¸âƒ£ Grafana Installation

On the Monitoring Server, I:

- Installed Grafana
- Enabled and started the Grafana service
- Accessed the Grafana UI on port 3000

### 2ï¸âƒ£ Prometheus as a Data Source

Inside Grafana:

- Added Prometheus as a data source
- Pointed it to the Prometheus server URL
- Verified successful connection

This connects metrics storage â†’ visualization.

### 3ï¸âƒ£ Importing Dashboards

Instead of building dashboards from scratch, I imported community-proven dashboards.

For Linux server metrics:

- Imported dashboard ID: 14513

This instantly gave me:

- CPU, memory, disk, and network graphs
- Per-node visibility
- Historical performance trends

---

## ğŸ”” Jenkins Monitoring

To monitor Jenkins:

- Installed the Prometheus metrics plugin in Jenkins
- Exposed Jenkins metrics at `/prometheus`
- Added Jenkins as a scrape target in Prometheus

After importing a Jenkins dashboard in Grafana, I could see:

- JVM memory usage
- Build executor load
- Job execution metrics

![Grafana Dashboard](assets/grafana.png)

---

## â˜¸ï¸ Kubernetes Monitoring

**Node-Level Monitoring:**

Since Node Exporter was running on both:

- Kubernetes Master
- Kubernetes Worker

I could monitor:

- Node health
- Resource usage
- System-level bottlenecks

Each node had its own Grafana dashboard.

![Prometheus Targets](assets/Prometheus.png)

---

## âœ… Final Result

![Application Live Output](assets/app-live-output.png)
